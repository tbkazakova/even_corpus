{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "txt2json.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Описание\n",
        "Берёт filename.txt\n",
        "\n",
        "- 1. AUTHOR_2021@textname_1\n",
        "- bu\ton'ariwun\tikənŋen...\t-\tfestival'du (первая строка после id строки)\n",
        "- bu\ton'a-ri-wun\tikən-ŋe-n\tfestival'-du (вторая строка)\n",
        "- Мы\tписать-PST-1PL\tпесня-DST-POSS.3SG\tфестиваль-DAT (третья строка)\n",
        "- ‘Мы написали песню для фестиваля.’ (четвёртая строка)\n",
        "- ...\n",
        "\n",
        "Дальше может быть что угодно, строка id вычисляется по re.match(r\"\\d+\\..*$\", line).\n",
        "\n",
        "Берёт meta из первой id строки.\n",
        "\n",
        "Записывает json файл в [формате tsakorpus](https://tsakorpus.readthedocs.io/en/latest/data_model.html).\n"
      ],
      "metadata": {
        "id": "OTbdGYCaUIil"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zGiixvpqBEFA"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pprint\n",
        "import re\n",
        "from itertools import zip_longest"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_meta(text, filename):\n",
        "    pattern = ' (.+?)_([0-9]+?)@(.+?)_([0-9]+)'\n",
        "    result = re.search(pattern, text)\n",
        "    filename = filename\n",
        "    meta = {\"filename\": filename,\n",
        "        \"author\": result.group(1),\n",
        "        \"year\": result.group(2),\n",
        "        \"recplace\": \"Bystraja district\",\n",
        "        \"title\": result.group(3),\n",
        "        \"genre\": \"newspaper\"}\n",
        "        # \"genre\": \"tale\"}\n",
        "    return meta"
      ],
      "metadata": {
        "id": "iJyJ7SKKkOQM"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PUNCTUATION = '!\"#$%&\\'()*+, -—./:;<=>?@[\\]^_`{|}~'"
      ],
      "metadata": {
        "id": "TdmsTILoBrbq"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filenames = [\"089_199702_1_eve_annotated\", \"089_199702_2_eve_annotated\"]"
      ],
      "metadata": {
        "id": "Wk08YCGMVJRJ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for filename in filenames:\n",
        "    eve_data = open(filename+'.txt').read()\n",
        "    sentences = []\n",
        "    sentences_ru = []\n",
        "    lines = eve_data.splitlines()\n",
        "    for line_id, line in enumerate(lines):\n",
        "        if re.match(r\"\\d+\\..*$\", line):\n",
        "            id = int(re.search(r\"\\d+\", line).group())  # id в тексте\n",
        "            if id == 1:\n",
        "                 meta = get_meta(line, filename)  # meta текста\n",
        "            text = lines[line_id+1]\n",
        "            words = text.split()  # слова в предложении\n",
        "            word_borders = []\n",
        "            word_borders.append(0)\n",
        "            for sym_id, symbol in enumerate(list(lines[line_id+1])):\n",
        "                if symbol.isspace():\n",
        "                    word_borders.append(sym_id-1)\n",
        "                    word_borders.append(sym_id+1)\n",
        "            word_borders.append(len(lines[line_id+1])-1)\n",
        "            chunks = list(x for x in zip_longest(*[iter(word_borders)] * 2))  # границы слов в предложении\n",
        "            parts_list = lines[line_id+2].split()  # список с морфемами предложения\n",
        "            gloss_list = lines[line_id+3].split()  # список с глоссами предложения\n",
        "        \n",
        "            gloss_index_list = []\n",
        "            for gl_parts in list(zip(gloss_list, parts_list)):\n",
        "                gloss_index = \"\"\n",
        "                for para in list(zip(gl_parts[0].split(\"-\"), gl_parts[1].split(\"-\"))):\n",
        "                    gloss_index += para[0]+\"{\"+ para[1] + \"}-\"\n",
        "                gloss_index_list.append(gloss_index)  # список с gloss_index предложения\n",
        "\n",
        "            for word_id, word in enumerate(words):\n",
        "                if word in PUNCTUATION:\n",
        "                    words.remove(word)\n",
        "                    chunks.pop(word_id)\n",
        "\n",
        "            words_info = []\n",
        "            for word_id, word in enumerate(words):\n",
        "                word_info = {'wf': word,\n",
        "                             'off_start': chunks[word_id][0],\n",
        "                             'off_end': chunks[word_id][1]+1,\n",
        "                             'wtype': 'word' if word not in PUNCTUATION else 'punct',  # что с тире делать\n",
        "                             'ana': [{'parts': parts_list[word_id],\n",
        "                                      'gloss': gloss_list[word_id],\n",
        "                                      'gloss_index': gloss_index_list[word_id]}],\n",
        "                              'next_word': word_id+1,\n",
        "                              'sentence_index': word_id,\n",
        "                              'sentence_index_neg': len(words)-word_id}\n",
        "                words_info.append(word_info)\n",
        "            sentence = {'text': text, 'words': words_info,\n",
        "                        \"lang\": 0, \"para_alignment\": [{\"off_start\": 0,\n",
        "                                                       \"off_end\": len(text),\n",
        "                                                       \"para_id\": id}]}\n",
        "            sentences.append(sentence)\n",
        "\n",
        "            text_ru = lines[line_id+4]\n",
        "            if text_ru != '':\n",
        "                words_ru = text_ru.split()  # слова в предложении\n",
        "                word_ru_borders = [0]\n",
        "                for sym_id, symbol in enumerate(list(lines[line_id+4])):\n",
        "                    if symbol.isspace():\n",
        "                        word_ru_borders.append(sym_id-1)\n",
        "                        word_ru_borders.append(sym_id+1)\n",
        "                word_ru_borders.append(len(lines[line_id+4])-1)\n",
        "                chunks = list(x for x in zip_longest(*[iter(word_ru_borders)] * 2))  # границы слов в предложении\n",
        "                words_ru_info = []\n",
        "                for word_ru_id, word_ru in enumerate(words_ru):\n",
        "                    word_ru_info = {'wf': word_ru,\n",
        "                                 'off_start': chunks[word_ru_id][0],\n",
        "                                 'off_end': chunks[word_ru_id][1]+1,\n",
        "                                 'wtype': 'word' if word_ru not in PUNCTUATION else 'punct',\n",
        "                                 'next_word': word_ru_id+1,\n",
        "                                 'sentence_index': word_ru_id,\n",
        "                                 'sentence_index_neg': len(words_ru)-word_ru_id}\n",
        "                    words_ru_info.append(word_ru_info)\n",
        "                sentence_ru = {\"text\": text_ru, \"words\": words_ru_info,\n",
        "                               \"lang\": 1, \"para_alignment\": [{\"off_start\": 0,\n",
        "                                                              \"off_end\": len(text_ru),\n",
        "                                                              \"para_id\": id}]}\n",
        "                sentences_ru.append(sentence_ru)\n",
        "    sentences.extend(sentences_ru)\n",
        "# Что со строкой комментариев? Её нет.\n",
        "    data = {'meta': meta,\n",
        "            'sentences': sentences}\n",
        "    jsonString = json.dumps(data, ensure_ascii=False)\n",
        "    with open(filename + '.json', 'w', encoding='utf-8') as f:\n",
        "        f.write(jsonString)"
      ],
      "metadata": {
        "id": "kmaN_8MqB34o"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data"
      ],
      "metadata": {
        "id": "oGFu4o5VCWOV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}